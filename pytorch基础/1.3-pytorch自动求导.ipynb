{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#（1） 简单情况的自动求导\n",
    "x=Variable(torch.Tensor([2]),requires_grad=True)\n",
    "y=x+2\n",
    "z=y**2+3\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#从z 对x 求导 为8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 更复杂的例子\n",
    "x=Variable(torch.randn(10,20),requires_grad=True)\n",
    "y=Variable(torch.randn(10,5),requires_grad=True)\n",
    "w=Variable(torch.randn(20,5),requires_grad=True)\n",
    "out=torch.mean(y-torch.matmul(x,w)) # torch.matmul 是做矩阵乘法\n",
    "out.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0205,  0.0497, -0.0509,  0.0060, -0.0283,  0.0112,  0.0205, -0.1640,\n",
      "         -0.0499,  0.0251, -0.0022,  0.0221,  0.0057, -0.0302,  0.0869, -0.0709,\n",
      "         -0.0377,  0.1421, -0.0272,  0.0669],\n",
      "        [-0.0205,  0.0497, -0.0509,  0.0060, -0.0283,  0.0112,  0.0205, -0.1640,\n",
      "         -0.0499,  0.0251, -0.0022,  0.0221,  0.0057, -0.0302,  0.0869, -0.0709,\n",
      "         -0.0377,  0.1421, -0.0272,  0.0669],\n",
      "        [-0.0205,  0.0497, -0.0509,  0.0060, -0.0283,  0.0112,  0.0205, -0.1640,\n",
      "         -0.0499,  0.0251, -0.0022,  0.0221,  0.0057, -0.0302,  0.0869, -0.0709,\n",
      "         -0.0377,  0.1421, -0.0272,  0.0669],\n",
      "        [-0.0205,  0.0497, -0.0509,  0.0060, -0.0283,  0.0112,  0.0205, -0.1640,\n",
      "         -0.0499,  0.0251, -0.0022,  0.0221,  0.0057, -0.0302,  0.0869, -0.0709,\n",
      "         -0.0377,  0.1421, -0.0272,  0.0669],\n",
      "        [-0.0205,  0.0497, -0.0509,  0.0060, -0.0283,  0.0112,  0.0205, -0.1640,\n",
      "         -0.0499,  0.0251, -0.0022,  0.0221,  0.0057, -0.0302,  0.0869, -0.0709,\n",
      "         -0.0377,  0.1421, -0.0272,  0.0669],\n",
      "        [-0.0205,  0.0497, -0.0509,  0.0060, -0.0283,  0.0112,  0.0205, -0.1640,\n",
      "         -0.0499,  0.0251, -0.0022,  0.0221,  0.0057, -0.0302,  0.0869, -0.0709,\n",
      "         -0.0377,  0.1421, -0.0272,  0.0669],\n",
      "        [-0.0205,  0.0497, -0.0509,  0.0060, -0.0283,  0.0112,  0.0205, -0.1640,\n",
      "         -0.0499,  0.0251, -0.0022,  0.0221,  0.0057, -0.0302,  0.0869, -0.0709,\n",
      "         -0.0377,  0.1421, -0.0272,  0.0669],\n",
      "        [-0.0205,  0.0497, -0.0509,  0.0060, -0.0283,  0.0112,  0.0205, -0.1640,\n",
      "         -0.0499,  0.0251, -0.0022,  0.0221,  0.0057, -0.0302,  0.0869, -0.0709,\n",
      "         -0.0377,  0.1421, -0.0272,  0.0669],\n",
      "        [-0.0205,  0.0497, -0.0509,  0.0060, -0.0283,  0.0112,  0.0205, -0.1640,\n",
      "         -0.0499,  0.0251, -0.0022,  0.0221,  0.0057, -0.0302,  0.0869, -0.0709,\n",
      "         -0.0377,  0.1421, -0.0272,  0.0669],\n",
      "        [-0.0205,  0.0497, -0.0509,  0.0060, -0.0283,  0.0112,  0.0205, -0.1640,\n",
      "         -0.0499,  0.0251, -0.0022,  0.0221,  0.0057, -0.0302,  0.0869, -0.0709,\n",
      "         -0.0377,  0.1421, -0.0272,  0.0669]])\n"
     ]
    }
   ],
   "source": [
    "# x 的梯度\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200],\n",
      "        [0.0200, 0.0200, 0.0200, 0.0200, 0.0200]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0375, -0.0375, -0.0375, -0.0375, -0.0375],\n",
       "        [ 0.0262,  0.0262,  0.0262,  0.0262,  0.0262],\n",
       "        [-0.0773, -0.0773, -0.0773, -0.0773, -0.0773],\n",
       "        [ 0.0150,  0.0150,  0.0150,  0.0150,  0.0150],\n",
       "        [ 0.1343,  0.1343,  0.1343,  0.1343,  0.1343],\n",
       "        [-0.0050, -0.0050, -0.0050, -0.0050, -0.0050],\n",
       "        [-0.0819, -0.0819, -0.0819, -0.0819, -0.0819],\n",
       "        [-0.0464, -0.0464, -0.0464, -0.0464, -0.0464],\n",
       "        [ 0.0414,  0.0414,  0.0414,  0.0414,  0.0414],\n",
       "        [-0.0839, -0.0839, -0.0839, -0.0839, -0.0839],\n",
       "        [-0.1112, -0.1112, -0.1112, -0.1112, -0.1112],\n",
       "        [-0.0129, -0.0129, -0.0129, -0.0129, -0.0129],\n",
       "        [-0.0228, -0.0228, -0.0228, -0.0228, -0.0228],\n",
       "        [ 0.0201,  0.0201,  0.0201,  0.0201,  0.0201],\n",
       "        [-0.0099, -0.0099, -0.0099, -0.0099, -0.0099],\n",
       "        [-0.0363, -0.0363, -0.0363, -0.0363, -0.0363],\n",
       "        [ 0.0701,  0.0701,  0.0701,  0.0701,  0.0701],\n",
       "        [-0.1042, -0.1042, -0.1042, -0.1042, -0.1042],\n",
       "        [-0.0278, -0.0278, -0.0278, -0.0278, -0.0278],\n",
       "        [ 0.0339,  0.0339,  0.0339,  0.0339,  0.0339]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 3.]], requires_grad=True)\n",
      "tensor([[0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#（2） 复杂情况的自动求导\n",
    "m=Variable(torch.FloatTensor([[2,3]]),requires_grad=True)  # 构建一个1*2 的矩阵\n",
    "n=Variable(torch.zeros(1,2)) 构建相同大小的0 矩阵\n",
    "print(m)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4., 27.]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# 通过m 中的值计算新的n 中的值\n",
    "n[0,0]=m[0,0]**2\n",
    "n[0,1]=m[0,1]**3\n",
    "print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4., 27.]])\n"
     ]
    }
   ],
   "source": [
    "# 在pytorch 中 如果要调用自动求导，需要往backward() 中传入一个参数，这个参数的形状和n 一样大\n",
    "n.backward(torch.ones_like(n)) #将(w0,w1) 取成(1,1)\n",
    "print(m.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#（3） 多次自动求导\n",
    "调用两次backward 就会发现程序报错，没有办法再做一次，这是因为pytorch 默认做完一次自动求导后，计算图就被丢弃了，所以两次自动求导需要手动设置一个东西\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=Variable(torch.FloatTensor([3]),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=x*2+x**2+3\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "y.backward(retain_graph=True) # 设置retain_graph 为True 来保留计算图\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() # 再做一次自动求导，这次不保留计算图\n",
    "x.grad\n",
    "# 这里的梯度变成了16，因为这里做了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13., 13.], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 练习 需要多看\n",
    "x=Variable(torch.FloatTensor([2,3]),requires_grad=True)\n",
    "x\n",
    "k=Variable(torch.zeros(2))\n",
    "k[0]=x[0]**2 +3*x[1]\n",
    "k[1]=x[1]**2+2*x[0]\n",
    "k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 3.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j=torch.zeros(2,2)\n",
    "k.backward(torch.FloatTensor([1,0]),retain_graph=True)\n",
    "j[0]=x.grad.data\n",
    "x.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 归0 之前求得的梯度\n",
    "x.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k.backward(torch.FloatTensor([0,1]))\n",
    "j[1]=x.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 3.],\n",
       "        [6., 9.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
